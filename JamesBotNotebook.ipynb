{"cells":[{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1625,"status":"ok","timestamp":1670464420238,"user":{"displayName":"Quentin Clark","userId":"06241829677215206003"},"user_tz":300},"id":"eV3jEpEcUqOa","outputId":"05d2be72-7f0a-41de-fa36-6ac37a31c18c"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n","data has 29343231 characters, 139 unique.\n"]}],"source":["# boiler plate shit \n","#!git clone https://github.com/optmlclass/PA2.git\n","import torch, torchvision\n","import torchvision.transforms as transforms\n","from torch.optim import Optimizer\n","import numpy as np\n","from tqdm import tqdm\n","from matplotlib import pyplot as plt\n","from collections.abc import Iterable\n","import os\n","# set up logging\n","import logging\n","logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO,\n",")\n","\n","# make deterministic\n","from PA2.sequenceutils import set_seed\n","set_seed(42)\n","\n","# imports for attention model\n","# more imports\n","from PA2.attentionmodel import GPT, GPTConfig\n","from PA2.sequenceutils import sample, CharDataset\n","from PA2.attentiontrainer import Trainer, TrainerConfig\n","\n","os.environ['CUDA_VISIBLE_DEVICES']='0'\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Assuming that we are on a CUDA machine, this should print a CUDA device:\n","print(device)\n","class AdamW_bias(Optimizer):\n","  def __init__(self, params, lr=1.0, betas=(0.9,0.999), weight_decay = 0.0, use_norm_scaling=False):\n","    super(AdamW_bias, self).__init__(params, {'lr': lr, 'beta1': betas[0], 'beta2': betas[1], 'weight_decay':weight_decay })\n","\n","\n","    epsilon = 1e-8\n","\n","    self.use_norm_scaling = use_norm_scaling\n","\n","    for group in self.param_groups:\n","      for p in group['params']:\n","        ## YOUR CODE HERE ##\n","\n","        state = self.state[p]\n","        state['step'] = 0\n","        state['mt'] = torch.zeros_like(p, device=p.device) # p as param should work?\n","        state['vt'] = torch.zeros_like(p, device=p.device)\n","\n","  @torch.no_grad()\n","  def step(self, closure=None):\n","    # in this class, and also usually in practice, closure will always be None.\n","    loss = None\n","    epsilon = 1e-8\n","    if closure is not None:\n","      with torch.enable_grad():\n","        loss = closure()\n","      \n","    ## YOUR CODE HERE ##\n","    for group in self.param_groups:\n","      lr = group['lr']\n","      beta1 = group['beta1']\n","      beta2 = group['beta2']\n","      weight_decay = group['weight_decay']\n","      \n","      for p in group['params']:\n","        if p.grad is None:\n","          continue\n","        state = self.state[p]\n","        state['step']+=1\n","        step = state['step']\n","\n","\n","        if self.use_norm_scaling: # aka 3 b\n","          # calcs all the relevant stuff\n","          gt = p.grad #+ weight_decay*p\n","          mt = (beta1*state['mt']) + ((1-beta1)*gt)\n","          vt = (beta2*state['vt']) + ((1-beta2)*torch.pow(gt,2))\n","\n","          # updates new param/weight\n","          big_thingy = (mt/(torch.sqrt(vt)+epsilon))+weight_decay*p\n","          p.add_(big_thingy, alpha=(-lr*torch.norm(p)))\n","\n","          # updates all the other stuff I forgot to \n","          state['mt'] = mt\n","          state['vt'] = vt\n","          \n","        else: # aka 3 a \n","\n","          # calcs all the relevant stuff\n","          gt = p.grad #+ weight_decay*p #this stuff might not be good?\n","          mt = (beta1*state['mt']) + ((1-beta1)*gt)\n","          vt = (beta2*state['vt']) + ((1-beta2)*torch.pow(gt,2))\n","          mt_hat = mt/(1-pow(beta1,step))\n","          vt_hat = vt/(1-pow(beta2,step))\n","\n","          # updates new param/weight\n","          big_thingy = (mt_hat/(torch.sqrt(vt_hat)+epsilon))+weight_decay*p\n","          p.add_(big_thingy, alpha=-lr)\n","\n","          # updates all the other stuff I forgot to \n","          #print('here is mt btw')\n","          #print(mt)\n","          state['mt'] = mt\n","          state['vt'] = vt\n","# generate training configurations for each of the optimizers. We will be testing\n","# adam (official pytorch implementation)\n","# adamw (official pytorch implementation)\n","# your optimizer both with and without the norm_scaling flag set.\n","def adamw_bias_factory(params, lr, betas):\n","  return AdamW_bias(params, lr, betas)\n","\n","def adamw_bias_norm_scaling_factory(params, lr, betas):\n","  return AdamW_bias(params, lr, betas, use_norm_scaling=True,weight_decay=5.5e-2) # MATHEMATICALLY INFORMED \n","# CHANGE BACK LATER \n","optimizers = {\n","    'adamw_bias': adamw_bias_factory,\n","    'adamw_bias_norm_scaling': adamw_bias_norm_scaling_factory,\n","    'adam': torch.optim.Adam, \n","    'adamw': torch.optim.AdamW\n","    #'adamw_bias': adamw_bias_factory,\n","    #'adamw_bias_norm_scaling': adamw_bias_norm_scaling_factory\n","  }\n","\n","training_configs = {}\n","# the \"block size\" is the number of characters the model takes as input.\n","# in this case, it can look at up to 128 characters when predicting the next\n","# character.\n","block_size = 384 # spatial extent of the model for its context\n","\n","# For our training set, we will use the text of the first four Harry Potter books.\n","text = open(\"/content/drive/MyDrive/henry_james.txt\", 'rb').read()\n","\n","train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters\n","for name, opt in optimizers.items():\n","\n","  training_configs[name] = TrainerConfig(max_epochs=1, batch_size=256, learning_rate=6e-4, optimizer=opt,\n","                        lr_decay=True, warmup_tokens=512*20, final_tokens=200*len(train_dataset)*block_size,\n","                        num_workers=2)\n","torch.cuda.empty_cache()\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3971,"status":"ok","timestamp":1670464256299,"user":{"displayName":"Quentin Clark","userId":"06241829677215206003"},"user_tz":300},"id":"0DPLAnVMaYt7","outputId":"c8cf3d7e-453b-4caa-9487-6051497bc39f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1670464407633,"user":{"displayName":"Quentin Clark","userId":"06241829677215206003"},"user_tz":300},"id":"C-dlBiOiaWj7","outputId":"a726e088-575c-4fd1-ce7b-dbe2f86a6e67"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n"]}],"source":["import os\n","print(os.getcwd())"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":335,"status":"ok","timestamp":1670464466845,"user":{"displayName":"Quentin Clark","userId":"06241829677215206003"},"user_tz":300},"id":"EFTg91uRPthh"},"outputs":[],"source":["# generate the configuration for the model. These parameters specify\n","# the neural network architecture we will be using. It is not necessary\n","# to understand this model architecture.\n","mconf_small = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n","                  n_layer=8, n_head=8, n_embd=128)\n","mconf_med = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n","                  n_layer=10, n_head=10, n_embd=260)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_gSApvJa_zte","outputId":"c807ba4e-28c6-41e6-9481-a67d709e068d"},"outputs":[{"data":{"text/plain":["3536"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["import gc\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CZpJC7xNZd8n","outputId":"1bd6f635-a046-4eed-9417-83497ce6339d"},"outputs":[{"name":"stdout","output_type":"stream","text":["training new model with optimizer: adamw\n","data has 29343231 characters, 139 unique.\n"]},{"name":"stderr","output_type":"stream","text":["epoch 1 iter 2381: train loss 0.92529. lr 4.999692e-07: 100%|██████████| 2382/2382 [24:08<00:00,  1.64it/s]\n","epoch 2 iter 2381: train loss 0.91176. lr 4.998767e-07: 100%|██████████| 2382/2382 [21:21<00:00,  1.86it/s]\n","epoch 3 iter 2381: train loss 0.91183. lr 4.997225e-07: 100%|██████████| 2382/2382 [20:39<00:00,  1.92it/s]\n","epoch 4 iter 2381: train loss 0.94828. lr 4.995068e-07: 100%|██████████| 2382/2382 [20:42<00:00,  1.92it/s]\n","epoch 5 iter 2381: train loss 0.93823. lr 4.992294e-07: 100%|██████████| 2382/2382 [20:46<00:00,  1.91it/s]\n","epoch 6 iter 2381: train loss 0.92567. lr 4.988906e-07: 100%|██████████| 2382/2382 [20:45<00:00,  1.91it/s]\n","epoch 7 iter 2381: train loss 0.90179. lr 4.984904e-07: 100%|██████████| 2382/2382 [20:43<00:00,  1.92it/s]\n","epoch 8 iter 2381: train loss 0.90764. lr 4.980288e-07: 100%|██████████| 2382/2382 [20:44<00:00,  1.91it/s]\n","epoch 9 iter 2381: train loss 0.91088. lr 4.975061e-07: 100%|██████████| 2382/2382 [20:43<00:00,  1.91it/s]\n","epoch 10 iter 2381: train loss 0.94096. lr 4.969223e-07: 100%|██████████| 2382/2382 [20:43<00:00,  1.92it/s]\n","epoch 11 iter 2381: train loss 0.92864. lr 4.962776e-07: 100%|██████████| 2382/2382 [20:44<00:00,  1.91it/s]\n","epoch 12 iter 2381: train loss 0.89647. lr 4.955721e-07: 100%|██████████| 2382/2382 [20:44<00:00,  1.91it/s]\n","epoch 13 iter 2381: train loss 0.92462. lr 4.948060e-07: 100%|██████████| 2382/2382 [20:46<00:00,  1.91it/s]\n","epoch 14 iter 2381: train loss 0.89596. lr 4.939795e-07: 100%|██████████| 2382/2382 [20:47<00:00,  1.91it/s]\n","epoch 15 iter 2381: train loss 0.95141. lr 4.930928e-07: 100%|██████████| 2382/2382 [20:47<00:00,  1.91it/s]\n","epoch 16 iter 2381: train loss 0.89375. lr 4.921461e-07: 100%|██████████| 2382/2382 [20:48<00:00,  1.91it/s]\n","epoch 17 iter 2381: train loss 0.88663. lr 4.911397e-07: 100%|██████████| 2382/2382 [20:49<00:00,  1.91it/s]\n","epoch 18 iter 2381: train loss 0.91404. lr 4.900738e-07: 100%|██████████| 2382/2382 [20:48<00:00,  1.91it/s]"]},{"name":"stdout","output_type":"stream","text":["final epoch train loss: 0.9183296606032227\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["## main training scripty :\n","best_optimizer = 'adamw_bias_norm_scaling'\n","long_train_conf = TrainerConfig(max_epochs=18, batch_size=32, learning_rate=5e-7, optimizer=optimizers[best_optimizer], # originally 6e-4\n","                        lr_decay=True, warmup_tokens=512*20, final_tokens=200*len(train_dataset)*block_size,\n","                        num_workers=2)\n","print(\"training new model with optimizer: {}\".format(name))\n","\n","train_dataset = CharDataset(text, block_size)\n","trainer = Trainer(best_model, train_dataset, None, long_train_conf)\n","train_loss = trainer.train()\n","print(\"final epoch train loss: {}\".format(train_loss))"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20688,"status":"ok","timestamp":1670464548754,"user":{"displayName":"Quentin Clark","userId":"06241829677215206003"},"user_tz":300},"id":"oE48jgJLPwvT","outputId":"a09ab1aa-36e8-4f24-836e-83b33f68a5b7"},"outputs":[{"name":"stdout","output_type":"stream","text":["I walked in on one brisk Thursday evening to my Henry James Class.\r\n","\r\n","I said to her that the Paris day was a confirmatory consideration, but that I had already been chattering a glance in the past took possession of her.  She was confidentially conscientious, but the sense that the past was a conclusion to the end of his confidence at any rate was to be confident to him that she could not have told him something of his conversation about the stage.  He was not of his own accepting the conversation, but he was surprised at the excursion that she was a good many years ago in the world; he was not sure that he should not have been so changing his career.  It was a stranger in this line that she had told him to be charmed and countries and a person.  She said nothing for anything, but she didnât say a word, âAre you going to tell him?â she asked, laughing.  âHe will say, in a way, anything but that he has got her away.â\r\n","\r\n","She turned away, but her eyes were still strange.  âAre you going to tell him about that?â she asked.\r\n","\r\n","âIt isnât he herself, if he doesnât, whether he doesnât like her.  He doesnât see her again or again as a sufficient one.â\r\n","\r\n","âHeâs going to say that, but with his sufficient proposal that he doesnât strike me as a painter.â\r\n","\r\n","Mrs. Penniman stared.  âI should like to see him in a manner that he doesnât know it.  And he doesnât care to marry me.â\r\n","\r\n","âHe should like it, my dear,â said Mrs. Penniman, âto send him a letter to me?  I donât know what he does in a word.â\r\n","\r\n","âHe doesnât know anything,â said Lord Lambeth.\r\n","\r\n","âI donât know what he does,â the young girl answered.  âHe sends me to Miss Tita,â she said, as if she had not heard himââand he has been awfully sorry for you.â\r\n","\r\n","âThat is what I meanââ  And he stared a moment at her companion with his eyes on the globe.\r\n","\r\n","âI mean to make it up,â she said.  âBut I mean that he has been the only person I shall have to, and that I have all my mind in proportion as yet.  I shall nev\n"]}],"source":["\n","context = [ord(c) for c in \"I walked in on one brisk Thursday evening to my Henry James Class.\"]\n","x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(device)\n","y = sample(best_model, x, 2000, temperature=0.9, sample=True, top_k=5)[0]\n","completion = ''.join([chr(train_dataset.itos[int(i)]) for i in y])\n","print(completion)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sGwWZ1Ppy0SW"},"outputs":[],"source":["#saves me fun model :)\n","torch.save(best_model.state_dict(),'henry_med.pth')"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6885,"status":"ok","timestamp":1670464488580,"user":{"displayName":"Quentin Clark","userId":"06241829677215206003"},"user_tz":300},"id":"R5p4qa1_Zd8q","outputId":"ea7d18ef-5384-486f-cf33-0e0583d67c47"},"outputs":[{"data":{"text/plain":["GPT(\n","  (tok_emb): Embedding(139, 260)\n","  (drop): Dropout(p=0.1, inplace=False)\n","  (blocks): Sequential(\n","    (0): Block(\n","      (ln1): LayerNorm((260,), eps=1e-05, elementwise_affine=True)\n","      (ln2): LayerNorm((260,), eps=1e-05, elementwise_affine=True)\n","      (attn): CausalSelfAttention(\n","        (key): Linear(in_features=260, out_features=260, bias=True)\n","        (query): Linear(in_features=260, out_features=260, bias=True)\n","        (value): Linear(in_features=260, out_features=260, bias=True)\n","        (attn_drop): Dropout(p=0.1, inplace=False)\n","        (resid_drop): Dropout(p=0.1, inplace=False)\n","        (proj): Linear(in_features=260, out_features=260, bias=True)\n","      )\n","      (mlp): Sequential(\n","        (0): Linear(in_features=260, out_features=1040, bias=True)\n","        (1): GELU(approximate='none')\n","        (2): Linear(in_features=1040, out_features=260, bias=True)\n","        (3): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (1): Block(\n","      (ln1): LayerNorm((260,), eps=1e-05, elementwise_affine=True)\n","      (ln2): LayerNorm((260,), eps=1e-05, elementwise_affine=True)\n","      (attn): CausalSelfAttention(\n","        (key): Linear(in_features=260, out_features=260, bias=True)\n","        (query): Linear(in_features=260, out_features=260, bias=True)\n","        (value): Linear(in_features=260, out_features=260, bias=True)\n","        (attn_drop): Dropout(p=0.1, inplace=False)\n","        (resid_drop): Dropout(p=0.1, inplace=False)\n","        (proj): Linear(in_features=260, out_features=260, bias=True)\n","      )\n","      (mlp): Sequential(\n","        (0): Linear(in_features=260, out_features=1040, bias=True)\n","        (1): GELU(approximate='none')\n","        (2): Linear(in_features=1040, out_features=260, bias=True)\n","        (3): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (2): Block(\n","      (ln1): LayerNorm((260,), eps=1e-05, elementwise_affine=True)\n","      (ln2): LayerNorm((260,), eps=1e-05, elementwise_affine=True)\n","      (attn): CausalSelfAttention(\n","        (key): Linear(in_features=260, out_features=260, bias=True)\n","        (query): Linear(in_features=260, out_features=260, bias=True)\n","        (value): Linear(in_features=260, out_features=260, bias=True)\n","        (attn_drop): Dropout(p=0.1, inplace=False)\n","        (resid_drop): Dropout(p=0.1, inplace=False)\n","        (proj): Linear(in_features=260, out_features=260, bias=True)\n","      )\n","      (mlp): Sequential(\n","        (0): Linear(in_features=260, out_features=1040, bias=True)\n","        (1): GELU(approximate='none')\n","        (2): Linear(in_features=1040, out_features=260, bias=True)\n","        (3): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (3): Block(\n","      (ln1): LayerNorm((260,), eps=1e-05, elementwise_affine=True)\n","      (ln2): LayerNorm((260,), eps=1e-05, elementwise_affine=True)\n","      (attn): CausalSelfAttention(\n","        (key): Linear(in_features=260, out_features=260, bias=True)\n","        (query): Linear(in_features=260, out_features=260, bias=True)\n","        (value): Linear(in_features=260, out_features=260, bias=True)\n","        (attn_drop): Dropout(p=0.1, inplace=False)\n","        (resid_drop): Dropout(p=0.1, inplace=False)\n","        (proj): Linear(in_features=260, out_features=260, bias=True)\n","      )\n","      (mlp): Sequential(\n","        (0): Linear(in_features=260, out_features=1040, bias=True)\n","        (1): GELU(approximate='none')\n","        (2): Linear(in_features=1040, out_features=260, bias=True)\n","        (3): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (4): Block(\n","      (ln1): LayerNorm((260,), eps=1e-05, elementwise_affine=True)\n","      (ln2): LayerNorm((260,), eps=1e-05, elementwise_affine=True)\n","      (attn): CausalSelfAttention(\n","        (key): Linear(in_features=260, out_features=260, bias=True)\n","        (query): Linear(in_features=260, out_features=260, bias=True)\n","        (value): Linear(in_features=260, out_features=260, bias=True)\n","        (attn_drop): Dropout(p=0.1, inplace=False)\n","        (resid_drop): Dropout(p=0.1, inplace=False)\n","        (proj): Linear(in_features=260, out_features=260, bias=True)\n","      )\n","      (mlp): Sequential(\n","        (0): Linear(in_features=260, out_features=1040, bias=True)\n","        (1): GELU(approximate='none')\n","        (2): Linear(in_features=1040, out_features=260, bias=True)\n","        (3): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (5): Block(\n","      (ln1): LayerNorm((260,), eps=1e-05, elementwise_affine=True)\n","      (ln2): LayerNorm((260,), eps=1e-05, elementwise_affine=True)\n","      (attn): CausalSelfAttention(\n","        (key): Linear(in_features=260, out_features=260, bias=True)\n","        (query): Linear(in_features=260, out_features=260, bias=True)\n","        (value): Linear(in_features=260, out_features=260, bias=True)\n","        (attn_drop): Dropout(p=0.1, inplace=False)\n","        (resid_drop): Dropout(p=0.1, inplace=False)\n","        (proj): Linear(in_features=260, out_features=260, bias=True)\n","      )\n","      (mlp): Sequential(\n","        (0): Linear(in_features=260, out_features=1040, bias=True)\n","        (1): GELU(approximate='none')\n","        (2): Linear(in_features=1040, out_features=260, bias=True)\n","        (3): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (6): Block(\n","      (ln1): LayerNorm((260,), eps=1e-05, elementwise_affine=True)\n","      (ln2): LayerNorm((260,), eps=1e-05, elementwise_affine=True)\n","      (attn): CausalSelfAttention(\n","        (key): Linear(in_features=260, out_features=260, bias=True)\n","        (query): Linear(in_features=260, out_features=260, bias=True)\n","        (value): Linear(in_features=260, out_features=260, bias=True)\n","        (attn_drop): Dropout(p=0.1, inplace=False)\n","        (resid_drop): Dropout(p=0.1, inplace=False)\n","        (proj): Linear(in_features=260, out_features=260, bias=True)\n","      )\n","      (mlp): Sequential(\n","        (0): Linear(in_features=260, out_features=1040, bias=True)\n","        (1): GELU(approximate='none')\n","        (2): Linear(in_features=1040, out_features=260, bias=True)\n","        (3): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (7): Block(\n","      (ln1): LayerNorm((260,), eps=1e-05, elementwise_affine=True)\n","      (ln2): LayerNorm((260,), eps=1e-05, elementwise_affine=True)\n","      (attn): CausalSelfAttention(\n","        (key): Linear(in_features=260, out_features=260, bias=True)\n","        (query): Linear(in_features=260, out_features=260, bias=True)\n","        (value): Linear(in_features=260, out_features=260, bias=True)\n","        (attn_drop): Dropout(p=0.1, inplace=False)\n","        (resid_drop): Dropout(p=0.1, inplace=False)\n","        (proj): Linear(in_features=260, out_features=260, bias=True)\n","      )\n","      (mlp): Sequential(\n","        (0): Linear(in_features=260, out_features=1040, bias=True)\n","        (1): GELU(approximate='none')\n","        (2): Linear(in_features=1040, out_features=260, bias=True)\n","        (3): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (8): Block(\n","      (ln1): LayerNorm((260,), eps=1e-05, elementwise_affine=True)\n","      (ln2): LayerNorm((260,), eps=1e-05, elementwise_affine=True)\n","      (attn): CausalSelfAttention(\n","        (key): Linear(in_features=260, out_features=260, bias=True)\n","        (query): Linear(in_features=260, out_features=260, bias=True)\n","        (value): Linear(in_features=260, out_features=260, bias=True)\n","        (attn_drop): Dropout(p=0.1, inplace=False)\n","        (resid_drop): Dropout(p=0.1, inplace=False)\n","        (proj): Linear(in_features=260, out_features=260, bias=True)\n","      )\n","      (mlp): Sequential(\n","        (0): Linear(in_features=260, out_features=1040, bias=True)\n","        (1): GELU(approximate='none')\n","        (2): Linear(in_features=1040, out_features=260, bias=True)\n","        (3): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (9): Block(\n","      (ln1): LayerNorm((260,), eps=1e-05, elementwise_affine=True)\n","      (ln2): LayerNorm((260,), eps=1e-05, elementwise_affine=True)\n","      (attn): CausalSelfAttention(\n","        (key): Linear(in_features=260, out_features=260, bias=True)\n","        (query): Linear(in_features=260, out_features=260, bias=True)\n","        (value): Linear(in_features=260, out_features=260, bias=True)\n","        (attn_drop): Dropout(p=0.1, inplace=False)\n","        (resid_drop): Dropout(p=0.1, inplace=False)\n","        (proj): Linear(in_features=260, out_features=260, bias=True)\n","      )\n","      (mlp): Sequential(\n","        (0): Linear(in_features=260, out_features=1040, bias=True)\n","        (1): GELU(approximate='none')\n","        (2): Linear(in_features=1040, out_features=260, bias=True)\n","        (3): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","  )\n","  (ln_f): LayerNorm((260,), eps=1e-05, elementwise_affine=True)\n","  (head): Linear(in_features=260, out_features=139, bias=False)\n",")"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# loads the model back up for RETRAINING\n","PATH = '/content/drive/MyDrive/henry_med.pth'\n","best_model = GPT(mconf_med)\n","best_model.load_state_dict(torch.load(PATH))\n","best_model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qOLSBCrYZd8q","outputId":"c907e00b-a746-4aff-aa3c-68253c75063d"},"outputs":[{"name":"stderr","output_type":"stream","text":["10/08/2022 23:00:59 - INFO - PA2.attentionmodel -   number of parameters: 8.318440e+06\n"]}],"source":["# loads up NEW model for training\n","best_model = GPT(mconf_med)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wy6Ofk4zZd8r","outputId":"47d06c53-31a8-4558-a4dc-825fb34e604f"},"outputs":[{"data":{"text/plain":["0.01"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["1e-2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gXaKagHxZd8s","outputId":"988239a7-2f6f-481c-c4a5-2f335ff3d3d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["device = torch.device(\n","            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        )\n","print(device)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.8.15 ('myenv')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"},"vscode":{"interpreter":{"hash":"e8f3f11bc313062e6429c4cddb8feda00ee661d0117a1b4dce5fd9831ec4971b"}}},"nbformat":4,"nbformat_minor":0}
